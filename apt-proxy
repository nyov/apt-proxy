#! /bin/sh
#
# Copyright Paul Russell <rusty@linuxcare.com.au>
# Released under the GPL version 2 or later
#	Hacked by Stephen Rothwell <sfr@linuxcare.com>
#
#	`perl!' -- Anton Blanchard
#	`I was joking!' -- Paul Mackerras
#	`Note that Rusty's name is at the top' -- Stephen Rothwell.
APT_PROXY_VERSION=1.1.1

# Before anything, save output in fd 3, and redirect others to arg1 if avail
APT_PROXY_LOGFILE=${1:-/dev/null}
exec 3>&1 1>>$APT_PROXY_LOGFILE 2>&1

CONFIG_FILE=/etc/apt-proxy/apt-proxy.conf

# Write a debug message to the log if debug is enabled
#	debug([string ...])
debug()
{
    [ -n "$DEBUG" ] && echo "$$": "$@" >> $APT_PROXY_LOGFILE
}

# Write a message to the log.
#	log([string ...])
log()
{
    echo "$@" >> $APT_PROXY_LOGFILE
}

# Write a http reply header
#	write_header(response code, [content type] [content length]
#			[last modified time] [content encoding]
#			[filename])
#
write_header()
{
    # apt sometimes barfs if this isn't in a single packet.
    # That cost me about a week of debugging 8(.
    WH_HEADER="HTTP/1.0 $1\r\nDate: `date -u -R`\r\nServer: Apt-proxy $APT_PROXY_VERSION\r\n"
    [ -n "$2" ] && WH_HEADER="${WH_HEADER}Content-Type: $2\r\n"
    [ -n "$3" ] && WH_HEADER="${WH_HEADER}Content-Length: $3\r\n"
    [ -n "$4" ] && WH_HEADER="${WH_HEADER}Last-Modified: $4\r\n"
    [ -n "$5" ] && WH_HEADER="${WH_HEADER}Content-Encoding: $5\r\n"
    [ -n "$6" ] && [ -n "$DEBUG" ] &&
	WH_HEADER="${WH_HEADER}X-You-Wanted: $6\r\n"

    # dd forces it to sane blocks.
    echo -n -e "$WH_HEADER\r\n" | dd 2>/dev/null >&3
}

# Write out bad config file HTTP response, and exits.
#	bad_config(args...)
bad_config()
{
    log Bad configuration "$@"
    mesg="<HTML><HEAD>\r
<TITLE>500 Bad configuration</TITLE>\r
</HEAD><BODY>\r
<H1>Bad configuration file</H1>\r
The configuration file $CONFIG_FILE is $@.<P>\r
<P>\r
</BODY></HTML>\r"
    write_header "500 Bad configuration" text/html `echo -e "$mesg" | wc -c`
    echo -e "$mesg" 1>&3
    exit 1
}

# Called from config script to add back ends.
#	add_backend(url-prefix, file-prefix, rsync-prefix...)
add_backend()
{
    [ $# -ge 3 ] || bad_config Bad add_backend "$@".
    APT_PROXY_BACKENDS="$APT_PROXY_BACKENDS`echo $@ | tr -s ' \t' ,` "
}

# Takes URL, reason, spits out 404 and exits.
#	bad_url(url, reason...)
bad_url()
{
    log Bad URL "$@"
    URL="$1"
    shift
    mesg="<HTML><HEAD>\r
<TITLE>404 Not Found</TITLE>\r
</HEAD><BODY>\r
<H1>Not Found</H1>\r
While serving the requested URL $URL: $@.<P>\r
Really.<P>\r
</BODY></HTML>\r"
    write_header "404 `echo \"$@\" | tail -1`" text/html
    echo -e "$mesg" 1>&3
    exit 1
}

# Relative or same directory is OK.
#	is_sane(symlink)
is_sane()
{
    case "$1" in ..*) return 0 ;; */*) return 1 ;; *) return 0; esac
}

# Tries all backends for a link.
#	rsync_link(linkname, backends...)
rsync_link()
{
    RSL_LINK=$1
    shift

    for rsl_back in $@; do
	if rsl_link="`rsync -l $rsl_back$RSL_LINK`"; then
	    # Ignore server messages.
	    echo "$rsl_link" | tail -1 | grep '^l.*->' | sed 's/.*-> //'
	    return 0
	fi
    done
    return 1
}

# Given a directory, print out all the parents in forwards order.
#	directories(pathname)
directories()
{
    DR_TOTAL=""
    for dr_d in `echo $1 | tr / ' '`; do
	DR_TOTAL="$DR_TOTAL/$dr_d"
	echo "$DR_TOTAL"
    done
}

# Make a directory tree, checking that the counterpart are real dirs
#	mkdir_maybe_symlinks(frontend base, subdir, backends...)
mkdir_maybe_symlinks()
{
    [ -d "$1$2" ] && return
    MMS_FRONTBASE="$1"
    MMS_SUBDIR="$2"
    shift 2

    for mms_d in `directories $MMS_SUBDIR`; do
	if [ ! -d "$MMS_FRONTBASE$mms_d" ]; then
	    MMS_L="`rsync_link \"$mms_d\" \"$@\"`" ||
		bad_url "$mms_d" "directory does not exist on any server"
	    if [ -n "$MMS_L" ]; then
		if is_sane "$MMS_L"; then
		    log Making symbolic link "$MMS_FRONTBASE$mms_d" '->' "$MMS_L"
		    rm -f "$MMS_FRONTBASE$mms_d"
		    ln -s "$MMS_L" "$MMS_FRONTBASE$mms_d"
		else
		    bad_url "$mms_d" "is a bad symlink $MMS_L on server"
		fi
		# Don't support nested directory symlinks.  Mail rusty.
		[ -d "$MMS_FRONTBASE$mms_d" ] ||
		    mkdir -m 755 "`dirname $MMS_FRONTBASE$mms_d`/$MMS_L"
	    else
		log Making directory "$MMS_FRONTBASE$mms_d"
		mkdir -m 755 "$MMS_FRONTBASE$mms_d"
	    fi
	    [ ! -d "$MMS_FRONTBASE$mms_d" ] &&
		bad_config "$MMS_FRONTBASE$mms_d" \
		    "is not a directory, and mkdir failed"
	fi
   done
}

# Make sure that any symlinks in the current path haven't changed.
#	check_symlinks(urlrest, frontend base, backends...)
check_symlinks()
{
    CS_SUBDIR="`basename $1`"
    CS_FRONTBASE="$2"
    shift 2

    for cs_d in `directories "$CS_SUBDIR"`; do
	if [ -L "$CS_FRONTBASE$cs_d" ]; then
	    CS_FRONTL="`readlink $CS_FRONTBASE$cs_d`"
	    # If this fails, simply abort check.
	    CS_BACKL="`rsync_link $cd_d $@`" || return
	    debug Link "$cs_d": mine "$CS_FRONTL" back "$CS_BACKL"
	    if [ "$CS_FRONTL" != "$CS_BACKL" ]; then
		log Updating link "$cs_d" from "$CS_FRONTL" to "$CS_BACKL"
		rm -f "$CS_FRONTBASE$cs_d"
		ln -s "$CS_BACKL" "$CS_FRONTBASE$cs_d"
	    fi
	fi
   done
}

# Given URL, sets $FRONT (local frontend file)
# Also FRONT_BASE, URL_REST, URL_SUBDIR and BACKS
# Creates directories up to $FRONT if neccessary.
# If iscontrolfile is set, then return backend they want control files
# to come from.
#	resolve_url(url, iscontrolfile)
resolve_url()
{
    for try in $APT_PROXY_BACKENDS; do
	PREFIX=`echo $try | cut -d, -f1`
	case "$1" in
	$PREFIX*)
	    # +1 for \n.
	    PREFIXLEN=`echo $PREFIX | wc -c`
	    URL_REST="`echo $1 | cut -c${PREFIXLEN}-`"
	    FRONT_BASE="`echo $try | cut -d, -f2`"
	    BACKS="`echo $try|cut -d, -f3-|tr , \\\n`"
	    if [ -n "$2" ] && echo "$BACKS" | grep -q '^+'; then
		BACKS="`echo \"$BACKS\" | grep '^+'` `echo \"$BACKS\" | grep -v '^+'`"
	    fi
	    # Remove bogus +'s.
	    BACKS="`echo \"$BACKS\" | sed s/^+//`"
	    FRONT="$FRONT_BASE$URL_REST"
	    URL_SUBDIR="`dirname \"$URL_REST\"`"
	    mkdir_maybe_symlinks "$FRONT_BASE" "$URL_SUBDIR" $BACKS
	    return
	;;
	esac
    done
    bad_url "$1" "is not serviced by this server"
}

# Let's not use procmail's lockfile; avoid dependency.
#	my_lockfile(lockfile)
my_lockfile()
{
    ret=1

    # noclobber
    set -C
    if echo $$ > $1; then ret=0
    else
	# Stale lock detection.
	if kill -0 "`cat $1`"; then :
	else
	    debug Lock $1 stale...
	    if ln $1 $1.stale 2>/dev/null; then
		# Someone may have cleaned this up between above lines.
		if kill -0 "`cat $1`"; then :
		else
		    echo $$ > "$1"
		    ret=0
		fi
	    fi
	    rm -f $1.stale
	fi
    fi 2>/dev/null
    set +C

    return $ret
}

# Find a decent match to base rsync on. 
#	copy_best_match(front partial)
copy_best_match()
{
    # Try to find a previous version of the same package.
    CBM_START=`echo $1 | cut -d_ -f1`
    CBM_EXT=`echo $1 | sed 's/^.*\.//'`

    CBM_BEST=`ls "$CBM_START"*."$CBM_EXT" 2> /dev/null | tail -1`
    debug Found best basis for ${1}: $CBM_BEST
    [ -n "$CBM_BEST" ] && cp $CBM_BEST $2
}

# Do rsync, drop lock.
#	rsync_unlock(front url-rest backends...)
rsync_unlock()
{
    RSU_FRONT=$1
    RSU_REST=$2
    shift 2
    RSU_PART=$RSU_FRONT.partial
    RSU_FAIL=$RSU_FRONT.fail
    RSU_OPTS="-v --perms --links --times --partial"

    # If file exists (eg. Packages file), copy it to partial.
    # In that case, don't get Packages if it's NEWER.
    if [ -f "$RSU_FRONT" ]; then
	cp -p $RSU_FRONT $RSU_PART
	RSU_OPTS="$RSU_OPTS --update"
    elif [ ! -f "$RSU_PART" ]; then
	copy_best_match $RSU_FRONT $RSU_PART
    fi
    for rsu_b in $@; do
	debug Trying $rsu_b...
	if rsync $RSU_OPTS $rsu_b$RSU_REST $RSU_PART >$RSU_FAIL 2>&1; then
	    mv $RSU_PART $RSU_FRONT
	    [ -n "$KEEP_STATS" ] && log $RSU_REST `tail -1 $RSU_FAIL`
	    rm -f $RSU_FAIL
	    break;
	fi
	debug `tail -1 < $RSU_FAIL`
    done
    rm -f $RSU_FRONT.lock
}

# Given filename, return wildcard which rsync would use.
#	rsync_wildcard(filename)
rsync_wildcard()
{
    echo `dirname $1`/.`basename $1`.partial."*"
}

# Returns when actual file (not symlink!) has begun rsync.
# If it has found a stream, returns stream name.
#	fetch_file_start(url-rest frontend-base backends...)
fetch_file_start()
{
    FF_URL_REST=$1
    FF_FRONT_BASE=$2
    shift 2

    FF_FRONT=$FF_FRONT_BASE$FF_URL_REST
    while true; do
	debug Fetching $FF_FRONT from $@

	# Need >/dev/null otherwise our caller (waiting for stdout)
	# waits for rsync_unlock to finish.
	my_lockfile $FF_FRONT.lock &&
	    (rsync_unlock $FF_FRONT $FF_URL_REST $@ &) >/dev/null

	FF_WILD="`rsync_wildcard $FF_FRONT`"
	while [ -f $FF_FRONT.lock ]; do
	    # A stream?
	    FF_STREAM="`echo $FF_WILD`"
	    if [ "$FF_STREAM" != "$FF_WILD" ]; then
		echo $FF_STREAM
		return 0
	    fi
	    sleep 1
	done

	# File, or non-dangling symlink?
	[ -f $FF_FRONT ] && return 0

	# Dangling symlink?  Follow.
	LINK="`readlink $FF_FRONT`"
	if [ -n "$LINK" ]; then
	    debug rsync gave a link...
	    if is_sane $LINK; then :
	    else
		log Bogus symlink $LINK from $@ rejected.
		return 1
	    fi
	    # Simply append to dirname.
	    FF_FRONT=`dirname $FF_FRONT`/$LINK
	    FF_URL_REST=`dirname $FF_URL_REST`/$LINK
	    mkdir_maybe_symlinks $FF_FRONT_BASE `dirname $FF_URL_REST` $@
	else
	    # rsync failed to give a result.
	    return 1
	fi
    done
}

# Can I suppress getting this file (too recent).
#	can_suppress(file timeout)
can_suppress()
{
    find $1 -cmin -$2 2>/dev/null | fgrep -q -x $1 && return 0
    find $1.nonexist -mmin -$2 2>/dev/null|fgrep -q -x $1.nonexist && return 0
    return 1
}

# Like fetch_file_start, but doesn't bother if within last BACKEND_FREQ.
#	fetch_file_maybe(url-rest frontend-base backends...)
fetch_file_maybe()
{
    if can_suppress $2$1 ${BACKEND_FREQ:-0}; then debug Suppressing $2$1
    else
	# This is expensive, only do for Packages (first file apt asks for)
	echo $1 | fgrep -q Packages && check_symlinks "$@"
	# Refresh nonexist.
	touch -c $2$1.nonexist 2>/dev/null
	fetch_file_start "$@"
    fi
}

# Wait for lock file to vanish; basically turns into synchronous mode.
#	wait_for_stream(file)
wait_for_stream()
{
    while [ -f $1.lock ]; do sleep 1; done
    # Return value based on whether file exists.
    [ -f $1 ]
}

# $1 may be null; if not, spool it out: it may turn into $2 at some stage.
#	stream_file(temporary-file, final-file)
stream_file()
{
    SF_OFFSET=0

    # wc -c handles symlinks
    while SF_LEN=`wc -c < "$1"`; do
	SF_COUNT=`expr $SF_LEN - $SF_OFFSET`
	if [ "$SF_COUNT" -gt 4096 ]; then
	    debug Doing $SF_COUNT bytes at $SF_OFFSET...
	    dd ibs=1 obs=16384 skip=$SF_OFFSET count=$SF_COUNT if="$1" >&3 ||
		break
	    # Remove the damn spaces `wc' puts in.
	    SF_OFFSET=`echo $SF_LEN`
	fi
	sleep 1
    done 2>/dev/null

    # Race: wait for lock to vanish (may have to move file)
    while [ -f $2.lock ]; do sleep 1; done

    # Dump the rest.
    if (dd bs=1 count=$SF_OFFSET of=/dev/null 2>/dev/null && cat) < $2 >&3
    then :
    else
	log Delivered partial file $2.
	exit 1
    fi
}

# Delete any expired files older than that just served.
#	cleanup_old(newfile,days)
cleanup_old()
{
    case $1 in
    *.deb) CL_EXT=.deb ;;
    *.tar.gz) CL_EXT=.tar.gz ;;
    *.dsc) CL_EXT=.dsc ;;
    *.diff.gz) CL_EXT=.diff.gz ;;
    esac

    # We're after dirname/foo_*.$CL_EXT
    CO_BASEDEB=`dirname $1`/`basename $1 | cut -d_ -f1`
    for CO_F in $CO_BASEDEB*$CL_EXT; do
	if dpkg --compare-versions $CO_F lt $1 && 
	    [ x"`find $CO_F -atime +$2 2>/dev/null`" != x"" ]
	then
	    log Cleaning up $CO_F
	    rm $CO_F
	fi
    done
}

# Clean up any debs which haven't been accessed in this many days.
#	sweep_clean_unlock(basedir days)
sweep_clean_unlock()
{
    SW_FILE=$1/.apt-proxy

    log Doing sweep of $1 in background...
    for f in `find $1 -atime +$2 -name '*.deb' -o -name '*.tar.gz' -o -name '*.dsc' -o -name '*.diff.gz'`; do
	log Sweeping clean $f
	rm $f
    done

    touch $SW_FILE
    rm -f $SW_FILE.lock
}

# Look back directories up to front-basedir until filename or
# filename.gz is found, and cat it.
#	find_and_cat(filename front-basedir subdir)
find_and_cat()
{
    # Could be a /binary-all/ (no Packages/Sources file).
    FAC_DIR=`echo $3 | sed s:/binary-all/:/binary-*/:`
    FAC_DIR=`echo $FAC_DIR | cut -d' ' -f1`
    
    for d in `directories $FAC_DIR | sort -r`; do
	debug Looking for $2$d/$1
	if [ -f $2/$d/$1 ]; then
	    debug Found $2/$d/$1
	    cat $2/$d/$1
	    return 0
	elif [ -f $2/$d/$1.gz ]; then
	    debug Found $2/$d/$1.gz
	    zcat $2/$d/$1.gz
	    return 0
        fi
    done
}

################################# MAIN START ##############################
# Read our configuration
#. /etc/apt-proxy/apt-proxy.conf
. $CONFIG_FILE

# For all the requests in a particular session
while true; do
    unset DO_CLEANUP SIZE_FILE IMS_DATE UNCOMPRESS REQUEST

    while read PREFIX LINE
    do
	LC_PREFIX="`echo $PREFIX | tr -d '\015' | tr '[A-Z]' '[a-z]'`"
	case "$LC_PREFIX" in
	get)
	    REQUEST="`echo $LINE | cut -d\  -f1 | sed 's/%5[Ff]/_/g'`"
	    ;;
	if-modified-since:)
	    IMS_DATE=$LINE
	    debug "If-Modified-Since $IMS_DATE"
	    IMS_DATE=`date -u -d "$IMS_DATE" '+%s'`
	    ;;
	'')
	    debug Finished processing request
	    break
	    ;;
	*)
	    debug Got line "$PREFIX $LINE"
	    ;;
	esac
    done

    INSANE="[^A-Za-z0-9_./+=:-]"
    echo "$REQUEST" | grep -q "$INSANE" &&
	bad_url "$REQUEST" Malformed chars:`echo \"$REQUEST\"|tr -dc "$INSANE"`

    [ -n "$REQUEST" ] && log `date` Request "$REQUEST"

    case $REQUEST in
    '')
	# Otherwise we send RSTs to some clients.
	sleep 1
	exit 0
	;;
    *..*)
	log Bad file requested $REQUEST
	exit 0
	;;
    # We always keep the Packages and Release files uptodate: derive the .gz.
    *Packages.gz|*Release.gz|*Sources.gz)
	# Rsync uncompressed files and derive.
	debug Updating compressed file \`$REQUEST\'
	FILE=`echo $REQUEST | sed 's/\.gz$//'`
	resolve_url $FILE 1

	# If we already have an uncompressed file, rsync it.
	if [ -f $FRONT ]; then
	    fetch_file_maybe $URL_REST $FRONT_BASE $BACKS > /dev/null
	    STREAM=""
	    # Can't stream, we need to compress.
	    if wait_for_stream $FRONT; then
		if [ ! -f $FRONT.gz -o $FRONT -nt $FRONT.gz ]; then
		    gzip -9 < $FRONT > $FRONT.gz
		    touch -r $FRONT $FRONT.gz
		fi
	    fi
	else
	    # Some unofficial backends don't do uncompressed Packages files
	    # and there's not usually an uncompressed Sources file.

	    # If this is first time around, derive uncompressed if it
	    # exists on a back end.
	    if [ ! -f $FRONT.gz ]; then
		for back in $BACKS; do
		    if rsync $back$URL_REST 2>/dev/null >/dev/null; then
			UNCOMPRESS=$FRONT
		    else
			log $back does not have `basename $FRONT`.
		    fi
		done
	    fi
	    STREAM="`fetch_file_maybe $URL_REST.gz $FRONT_BASE $BACKS`"
	fi
	FRONT=$FRONT.gz

	contype=text/plain
	conenc=x-gzip
	;;

    *Packages|*Release|*Sources)
	debug Updating file \`$REQUEST\'
	resolve_url $REQUEST 1
	STREAM="`fetch_file_maybe $URL_REST $FRONT_BASE $BACKS`"

	contype=text/plain
	conenc=
	;;

    *.deb)
	debug Looking for file \`$REQUEST\'
	resolve_url $REQUEST
	[ -f $FRONT ] ||
	    STREAM="`fetch_file_start $URL_REST $FRONT_BASE $BACKS`"
	debug STREAM: $STREAM
	[ -n "$CLEANUP_DAYS" ] && DO_CLEANUP=1

	SIZE_FILE=Packages
	contype=application/dpkg
	conenc=
	;;

    *.diff.gz|*.tar.gz)
	resolve_url $REQUEST
	[ -f $FRONT ] ||
	    STREAM="`fetch_file_start $URL_REST $FRONT_BASE $BACKS`"
	[ -n "$CLEANUP_DAYS" ] && DO_CLEANUP=1

	SIZE_FILE=Sources
	contype=application/x-gzip
	conenc=
	;;

    *.dsc)
	resolve_url $REQUEST
	[ -f $FRONT ] ||
	    STREAM="`fetch_file_start $URL_REST $FRONT_BASE $BACKS`"
	[ -n "$CLEANUP_DAYS" ] && DO_CLEANUP=1

	SIZE_FILE=Sources
	contype=text/plain
	conenc=
	;;

    *)
	bad_url $REQUEST Unknown extension.
	;;
    esac

    # Beware race.
    if [ -f "$STREAM" ]; then
	rm -f $FRONT.nonexist
	if [ -n "$SIZE_FILE" ]; then
	    # Damn.  Don't hit server again to get length: backtrace and find.
	    BASEFILE_NAME=`basename $REQUEST`
	    case $SIZE_FILE in
	    Packages)
		SIZE=`find_and_cat Packages $FRONT_BASE $URL_SUBDIR | grep-dctrl -e -F Filename "/$BASEFILE_NAME$" -s Size | cut -d\  -f2 | head -1`
		;;
	    Sources)
		PACKAGE_NAME="`basename $REQUEST | cut -d_ -f1`"
		SIZE=`find_and_cat Sources $FRONT_BASE $URL_SUBDIR | grep-dctrl -X -P  "$PACKAGE_NAME" -s Files | fgrep $BASEFILE_NAME | cut -d\  -f3 | head -1`
		;;
	    esac
	    debug Derived size: $SIZE
	fi
    else
	# Race: wait for lock to vanish (may have to move file)
	while [ -f $FRONT.lock ]; do sleep 1; done

	if [ -f $FRONT ]; then
	    # File already exists in entirity.  Give local answers.
	    rm -f $FRONT.nonexist
	    # wc -c avoids symlink problem.
	    SIZE=`wc -c < $FRONT`
	    DATE="`date -u -R -r $FRONT`"
	    # Turn date into seconds. 
	    FILE_DATE=`date -u '+%s' -d "$DATE"`
	    if [ -n "$IMS_DATE" ] && [ $IMS_DATE -ge $FILE_DATE ]; then
		write_header "304 HIT"
		continue
	    fi
	else
	    # Golly!
	    CAUSE=`fgrep -v -x 'client: nothing to do' $FRONT.fail 2>/dev/null`
	    rm -f $FRONT.fail
	    # Often Release files really don't exist: cache the `miss'.
	    case "$FRONT" in *Release|*Release.gz)
		[ ! -f $FRONT.nonexist ] && touch $FRONT.nonexist;;
	    esac
	    bad_url $REQUEST "${CAUSE:-was not found on this server}"
	fi
    fi

    write_header "200 OK" "$contype" "$SIZE" "$DATE" "$conenc" "$REQUEST"
    stream_file "$STREAM" "$FRONT"

    # Now, derive the uncompressed file.
    if [ -n "$UNCOMPRESS" ]; then
	gunzip < $FRONT > $UNCOMPRESS
	touch -r $FRONT $UNCOMPRESS
    fi

    [ -n "$DO_CLEANUP" ] && cleanup_old $FRONT $CLEANUP_DAYS

    # Do async once in a very long while.
    if [ -n "$CLEAN_SWEEP" ]; then
	if [ ! -f $FRONT_BASE/.apt-proxy ] ||
	    find $FRONT_BASE/.apt-proxy -mtime +$CLEAN_SWEEP |
		fgrep -q -x $FRONT_BASE/.apt-proxy
	then
	    my_lockfile $FRONT_BASE/.apt-proxy &&
		sweep_clean_unlock $FRONT_BASE $CLEAN_SWEEP &
	fi
    fi
    if [ -z "$SIZE" ]; then log Aborting: Unknown size for $REQUEST; break; fi
done
