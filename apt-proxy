#! /bin/sh
#
# Copyright Paul Russell <rusty@linuxcare.com.au>
# Released under the GPL version 2 or later
#	Hacked by Stephen Rothwell <sfr@linuxcare.com>
#
#	`perl!' -- Anton Blanchard
#	`I was joking!' -- Paul Mackerras
#	`Note that Rusty's name is at the top' -- Stephen Rothwell.
APT_PROXY_VERSION=1.0.0

# Before anything, save output in fd 3, and redirect others to arg1 if avail
APT_PROXY_LOGFILE=${1:-/dev/null}
exec 3>&1 1>>$APT_PROXY_LOGFILE 2>&1

CONFIG_FILE=/etc/apt-proxy/apt-proxy.conf

# Write a debug message to the log if debug is enabled
#	debug([string ...])
debug()
{
    [ -n "$DEBUG" ] && echo "$$": "$@" >> $APT_PROXY_LOGFILE
}

# Write a message to the log.
#	log([string ...])
log()
{
    echo "$@" >> $APT_PROXY_LOGFILE
}

# Write a http reply header
#	write_header(response code, [content type] [content length]
#			[last modified time] [content encoding]
#			[filename])
#
write_header()
{
    # apt sometimes barfs if this isn't in a single packet.
    # That cost me about a week of debugging 8(.
    WH_HEADER="HTTP/1.0 $1\r\nDate: `date -u -R`\r\nServer: Apt-proxy $APT_PROXY_VERSION\r\n"
    [ -n "$2" ] && WH_HEADER="${WH_HEADER}Content-Type: $2\r\n"
    [ -n "$3" ] && WH_HEADER="${WH_HEADER}Content-Length: $3\r\n"
    [ -n "$4" ] && WH_HEADER="${WH_HEADER}Last-Modified: $4\r\n"
    [ -n "$5" ] && WH_HEADER="${WH_HEADER}Content-Encoding: $5\r\n"
    [ -n "$6" ] && [ -n "$DEBUG" ] &&
	WH_HEADER="${WH_HEADER}X-You-Wanted: $6\r\n"

    # dd forces it to sane blocks.
    echo -n -e "$WH_HEADER\r\n" | dd 2>/dev/null >&3
}

# Write out bad config file HTTP response, and exits.
#	bad_config(args...)
bad_config()
{
    log Bad configuration "$@"
    mesg="<HTML><HEAD>\r
<TITLE>500 Bad configuration</TITLE>\r
</HEAD><BODY>\r
<H1>Bad configuration file</H1>\r
The configuration file $CONFIG_FILE is $@.<P>\r
<P>\r
</BODY></HTML>\r"
    write_header "500 Bad configuration" text/html `echo -e "$mesg" | wc -c`
    echo -e "$mesg" 1>&3
    exit 1
}

# Called from config script to add back ends.
#	add_backend(url-prefix, file-prefix, rsync-prefix...)
add_backend()
{
    [ $# -ge 3 ] || bad_config Bad add_backend "$@".
    APT_PROXY_BACKENDS="$APT_PROXY_BACKENDS`echo $@ | tr -s ' \t' ,` "
}

# Takes URL, reason, spits out 404 and exits.
#	bad_url(url, reason...)
bad_url()
{
    log Bad URL "$@"
    URL="$1"
    shift
    mesg="<HTML><HEAD>\r
<TITLE>404 Not Found</TITLE>\r
</HEAD><BODY>\r
<H1>Not Found</H1>\r
While serving the requested URL $URL: $@.<P>\r
Really.<P>\r
</BODY></HTML>\r"
    write_header "404 `echo \"$@\" | tail -1`" text/html
    echo -e "$mesg" 1>&3
    exit 1
}

# Relative or same directory is OK.
#	is_sane(symlink)
is_sane()
{
    case "$1" in ..*) return 0 ;; */*) return 1 ;; *) return 0; esac
}

# Tries all backends for a link.
#	rsync_link(linkname, backends...)
rsync_link()
{
    RSL_LINK=$1
    shift

    for rsl_back in $@; do
	if rsl_link="`rsync -l $rsl_back$RSL_LINK`"; then
	    # Ignore server messages.
	    echo "$rsl_link" | tail -1 | grep '^l.*->' | sed 's/.*-> //'
	    return 0
	fi
    done
    return 1
}

# Given a directory, print out all the parents in forwards order.
#	directories(pathname)
directories()
{
    DR_DIR="`echo $1 | cut -d/ -f1`"
    DR_REST="`echo $1 | cut -d/ -f2-`"
    while [ "$DR_DIR" != "$1/$DR_REST" ]; do
	echo "$DR_DIR"
	DR_DIR="$DR_DIR/`echo $DR_REST | cut -d/ -f1`"
	DR_REST="`echo $DR_REST | cut -d/ -f2-`"
    done
}

# Make a directory tree, checking that the counterpart are real dirs
#	mkdir_maybe_symlinks(frontend base, subdir, backends...)
mkdir_maybe_symlinks()
{
    [ -d "$1$2" ] && return
    MMS_FRONTBASE="$1"
    MMS_SUBDIR="$2"
    shift 2

    for mms_d in `directories $MMS_SUBDIR`; do
	if [ ! -d "$MMS_FRONTBASE$mms_d" ]; then
	    MMS_L="`rsync_link \"$mms_d\" \"$@\"`" ||
		bad_url "$mms_d" "directory does not exist on any server"
	    if [ -n "$MMS_L" ]; then
		if is_sane "$MMS_L"; then
		    log Making symbolic link "$MMS_FRONTBASE$mms_d" '->' "$MMS_L"
		    rm -f "$MMS_FRONTBASE$mms_d"
		    ln -s "$MMS_L" "$MMS_FRONTBASE$mms_d"
		else
		    bad_url "$mms_d" "is a bad symlink $MMS_L on server"
		fi
		# Don't support nested directory symlinks.  Mail rusty.
		[ -d "$MMS_FRONTBASE$mms_d" ] ||
		    mkdir -m 755 "`dirname $MMS_FRONTBASE$mms_d`/$MMS_L"
	    else
		log Making directory "$MMS_FRONTBASE$mms_d"
		mkdir -m 755 "$MMS_FRONTBASE$mms_d"
	    fi
	    [ ! -d "$MMS_FRONTBASE$mms_d" ] &&
		bad_config "$MMS_FRONTBASE$mms_d" \
		    "is not a directory, and mkdir failed"
	fi
   done
}

# Make sure that any symlinks in the current path haven't changed.
#	check_symlinks(urlrest, frontend base, backends...)
check_symlinks()
{
    CS_SUBDIR="`basename $1`"
    CS_FRONTBASE="$2"
    shift 2

    for cs_d in `directories "$CS_SUBDIR"`; do
	if [ -L "$CS_FRONTBASE$cs_d" ]; then
	    CS_FRONTL="`readlink $CS_FRONTBASE$cs_d`"
	    # If this fails, simply abort check.
	    CS_BACKL="`rsync_link $cd_d $@`" || return
	    debug Link "$cs_d": mine "$CS_FRONTL" back "$CS_BACKL"
	    if [ "$CS_FRONTL" != "$CS_BACKL" ]; then
		log Updating link "$cs_d" from "$CS_FRONTL" to "$CS_BACKL"
		rm -f "$CS_FRONTBASE$cs_d"
		ln -s "$CS_BACKL" "$CS_FRONTBASE$cs_d"
	    fi
	fi
   done
}

# Given URL, sets $FRONT (local frontend file)
# Also FRONT_BASE, URL_REST, URL_SUBDIR and BACKS
# Creates directories up to $FRONT if neccessary.
#	resolve_url(url)
resolve_url()
{
    for try in $APT_PROXY_BACKENDS; do
	PREFIX=`echo $try | cut -d, -f1`
	case "$1" in
	$PREFIX*)
	    # +1 for \n.
	    PREFIXLEN=`echo $PREFIX | wc -c`
	    URL_REST="`echo $1 | cut -c${PREFIXLEN}-`"
	    debug Resolved "$1" to "$try" \(rest = "$URL_REST"\)
	    FRONT_BASE="`echo $try | cut -d, -f2`"
	    BACKS="`echo $try | cut -d, -f3- | tr , ' '`"
	    FRONT="$FRONT_BASE$URL_REST"
	    URL_SUBDIR="`dirname \"$URL_REST\"`"
	    mkdir_maybe_symlinks "$FRONT_BASE" "$URL_SUBDIR" $BACKS
	    return
	;;
	esac
    done
    bad_url "$1" "is not serviced by this server"
}

# Let's not use procmail's lockfile; avoid dependency.
#	my_lockfile(lockfile)
my_lockfile()
{
    ret=1

    # noclobber
    set -C
    if echo $$ > $1; then ret=0
    else
	# Stale lock detection.
	if kill -0 "`cat $1`"; then :
	else
	    debug Lock $1 stale...
	    if ln $1 $1.stale 2>/dev/null; then
		# Someone may have cleaned this up between above lines.
		if kill -0 "`cat $1`"; then :
		else
		    echo $$ > "$1"
		    ret=0
		fi
	    fi
	    rm -f $1.stale
	fi
    fi
    set +C

    return $ret
}

# Do rsync, drop lock.
#	rsync_unlock(front url-rest backends...)
rsync_unlock()
{
    RSU_FRONT=$1
    RSU_REST=$2
    shift 2
    RSU_PART=$RSU_FRONT.partial
    RSU_FAIL=$RSU_FRONT.fail
    RSU_OPTS="--perms --links --times --partial"

    # If file exists (eg. Packages file), copy it to partial.
    # In that case, don't get Packages if it's NEWER.
    cp -p $RSU_FRONT $RSU_PART 2>/dev/null && RSU_OPTS="$RSU_OPTS --update"
    for rsu_b in $@; do
	debug Trying $rsu_b...
	if rsync $RSU_OPTS $rsu_b$RSU_REST $RSU_PART >$RSU_FAIL 2>&1; then
	    mv $RSU_PART $RSU_FRONT
	    rm -f $RSU_FAIL
	    break;
	fi
    done
    rm -f $RSU_FRONT.lock
}

# Given filename, return wildcard which rsync would use.
#	rsync_wildcard(filename)
rsync_wildcard()
{
    echo `dirname $1`/.`basename $1`.partial."*"
}

# Returns when actual file (not symlink!) has begun rsync.
# If it has found a stream, returns stream name.
#	fetch_file_start(url-rest frontend-base backends...)
fetch_file_start()
{
    FF_URL_REST=$1
    FF_FRONT_BASE=$2
    shift 2

    FF_FRONT=$FF_FRONT_BASE$FF_URL_REST
    while true; do
	debug Fetching $FF_FRONT from $@

	# Need >/dev/null otherwise our caller (waiting for stdout)
	# waits for rsync_unlock to finish.
	my_lockfile $FF_FRONT.lock &&
	    rsync_unlock $FF_FRONT $FF_URL_REST $@ >/dev/null &

	FF_WILD="`rsync_wildcard $FF_FRONT`"
	while [ -f $FF_FRONT.lock ]; do
	    # A stream?
	    FF_STREAM="`echo $FF_WILD`"
	    if [ "$FF_STREAM" != "$FF_WILD" ]; then
		echo $FF_STREAM
		return 0
	    fi
	    sleep 1
	done

	# File, or non-dangling symlink?
	[ -f $FF_FRONT ] && return 0

	# Dangling symlink?  Follow.
	LINK="`readlink $FF_FRONT`"
	if [ -n "$LINK" ]; then
	    debug rsync gave a link...
	    if is_sane $LINK; then :
	    else
		log Bogus symlink $LINK from $@ rejected.
		return 1
	    fi
	    # Simply append to dirname.
	    FF_FRONT=`dirname $FF_FRONT`/$LINK
	    FF_URL_REST=`dirname $FF_URL_REST`/$LINK
	    mkdir_maybe_symlinks $FF_FRONT_BASE `dirname $FF_URL_REST` $@
	else
	    # rsync failed to give a result.
	    return 1
	fi
    done
}

# Can I suppress getting this file (too recent).
#	can_suppress(file timeout)
can_suppress()
{
    find $1 -cmin -$2 2>/dev/null | fgrep -q -x $1
}

# Like fetch_file_start, but doesn't bother if within last BACKEND_FREQ.
#	fetch_file_maybe(url-rest frontend-base backends...)
fetch_file_maybe()
{
    if can_suppress $2$1 ${BACKEND_FREQ:-0}; then debug Suppressing $2$1
    else
	# This is expensive, only do for Packages (first file apt asks for)
	echo $1 | fgrep Packages && check_symlinks "$@"
	fetch_file_start "$@"
    fi
}

# Wait for lock file to vanish; basically turns into synchronous mode.
#	wait_for_stream(file)
wait_for_stream()
{
    while [ -f $1.lock ]; do sleep 1; done
    # Return value based on whether file exists.
    [ -f $1 ]
}

# $1 may be null; if not, spool it out: it may turn into $2 at some stage.
#	stream_file(temporary-file, final-file)
stream_file()
{
    SF_OFFSET=0

    # wc -c handles symlinks
    while SF_LEN=`wc -c < "$1"`; do
	SF_COUNT=`expr $SF_LEN - $SF_OFFSET`
	if [ "$SF_COUNT" -gt 4096 ]; then
	    debug Doing $SF_COUNT bytes at $SF_OFFSET...
	    dd ibs=1 obs=16384 skip=$SF_OFFSET count=$SF_COUNT if="$1" >&3 ||
		break
	    # Remove the damn spaces `wc' puts in.
	    SF_OFFSET=`echo $SF_LEN`
	fi
	sleep 1
    done 2>/dev/null

    # Race: wait for lock to vanish (may have to move file)
    while [ -f $2.lock ]; do sleep 1; done

    # Dump the rest.
    if (dd bs=1 count=$SF_OFFSET of=/dev/null 2>/dev/null && cat) < $2 >&3
    then :
    else
	log Delivered partial file $2.
	exit 1
    fi
}

# Delete any expired files older than that just served.
#	cleanup_old(newfile,days)
cleanup_old()
{
    case $1 in
    *.deb) CL_EXT=.deb ;;
    *.tar.gz) CL_EXT=.tar.gz ;;
    *.dsc) CL_EXT=.dsc ;;
    *.diff.gz) CL_EXT=.diff.gz ;;
    esac

    # We're after dirname/foo_*.$CL_EXT
    CO_BASEDEB=`dirname $1`/`basename $1 | cut -d_ -f1`
    for CO_F in $CO_BASEDEB*$CL_EXT; do
	if dpkg --compare-versions $CO_F lt $1 && 
	    [ x"`find $CO_F -atime +$2 2>/dev/null`" != x"" ]
	then
	    log Cleaning up $CO_F
	    rm $CO_F
	fi
    done
}

# Clean up any debs which haven't been accessed in this many days.
#	sweep_clean_unlock(basedir days)
sweep_clean_unlock()
{
    SW_FILE=$1/.apt-proxy

    log Doing sweep of $1 in background...
    for f in `find $1 -atime +$2 -name '*.deb' -o -name '*.tar.gz' -o -name '*.dsc' -o -name '*.diff.gz'`; do
	log Sweeping clean $f
	rm $f
    done

    touch $SW_FILE
    rm -f $SW_FILE.lock
}

# Look back directories up to front-basedir until filename or
# filename.gz is found, and cat it.
#	find_and_cat(filename front-basedir subdir)
find_and_cat()
{
    FAC_DIR=$2$3/
    FAC_STOP=`dirname $2`
    while [ $FAC_DIR != $FAC_STOP ]; do
	if [ -f $FAC_DIR$1 ]; then
	    debug Found $FAC_DIR$1
	    cat $FAC_DIR$1
	    return 0
	elif [ -f $FAC_DIR$1.gz ]; then
	    debug Found $FAC_DIR$1.gz
	    zcat $FAC_DIR$1.gz
	    return 0
        fi
	FAC_DIR=`echo $FAC_DIR | sed 's:/[^/]*/$:/:'`
    done
}

################################# MAIN START ##############################
# Read our configuration
#. /etc/apt-proxy/apt-proxy.conf
. $CONFIG_FILE

# Ignore that keepalive crap.  I'm an sh script, not a miracle worker, dammit!
while read PREFIX LINE
do
    LC_PREFIX="`echo $PREFIX | tr -d '\015' | tr '[A-Z]' '[a-z]'`"
    case "$LC_PREFIX" in
    get)
	REQUEST="`echo $LINE | cut -d\  -f1 | sed 's/%5[Ff]/_/g'`"
	;;
    if-modified-since:)
	IMS_DATE=$LINE
	debug "If-Modified-Since $IMS_DATE"
	IMS_DATE=`date -u -d "$IMS_DATE" '+%s'`
	;;
    '')
	debug Finished processing request
	break
	;;
    *)
	debug Got line "$PREFIX $LINE"
	;;
    esac
done

INSANE="[^A-Za-z0-9_./+=:-]"
echo "$REQUEST" | grep -q "$INSANE" &&
    bad_url "$REQUEST" Malformed chars: `echo \"$REQUEST\" | tr -d \"$INSANE\"`

log `date` Request "$REQUEST"

case $REQUEST in
'')
    log Empty request $REQUEST ignored.
    exit 0
    ;;
*..*)
    log Bad file requested $REQUEST
    exit 0
    ;;
# We always keep the Packages and Release files uptodate: derive the .gz.
*Packages.gz|*Release.gz|*Sources.gz)
    # Rsync uncompressed files and derive.
    debug Updating compressed file \`$REQUEST\'
    FILE=`echo $REQUEST | sed 's/\.gz$//'`
    resolve_url $FILE

    # If we already have an uncompressed file, rsync it.
    if [ -f $FRONT ]; then
	STREAM="`fetch_file_maybe $URL_REST $FRONT_BASE $BACKS`"
	# Can't stream, we need to compress.
	if wait_for_stream $FRONT; then
	    STREAM=""
	    if [ ! -f $FRONT.gz -o $FRONT -nt $FRONT.gz ]; then
		gzip -9 < $FRONT > $FRONT.gz
		touch -r $FRONT $FRONT.gz
	    fi
	fi
    else
	# Some unofficial backends don't do uncompressed Packages files
	# and there's not usually an uncompressed Sources file.

	# If this is first time around, derive uncompressed if it exists on
	# a back end.
	if [ ! -f $FRONT.gz ]; then
	    for back in $BACKS; do
		if rsync $back$URL_REST 2>/dev/null >/dev/null; then
		    UNCOMPRESS=$FRONT
		else
		    log $back does not have `basename $FRONT`.
		fi
	    done
	fi
        STREAM="`fetch_file_maybe $URL_REST.gz $FRONT_BASE $BACKS`"
    fi
    FRONT=$FRONT.gz

    contype=text/plain
    conenc=x-gzip
    ;;

*Packages|*Release|*Sources)
    debug Updating file \`$REQUEST\'
    resolve_url $REQUEST
    STREAM="`fetch_file_maybe $URL_REST $FRONT_BASE $BACKS`"

    contype=text/plain
    conenc=
    ;;

*.deb)
    debug Looking for file \`$REQUEST\'
    resolve_url $REQUEST
    # Only if it doesn't exist.
    [ -f $FRONT ] ||
	STREAM="`fetch_file_start $URL_REST $FRONT_BASE $BACKS`"
    [ -n "$CLEANUP_DAYS" ] && DO_CLEANUP=1

    SIZE_FILE=Packages
    contype=application/dpkg
    conenc=
    ;;

*.diff.gz|*.tar.gz)
    resolve_url "$REQUEST"
    # Only if it doesn't exist.
    [ -f $FRONT ] || STREAM="`fetch_file_start $URL_REST $FRONT_BASE $BACKS`"
    [ -n "$CLEANUP_DAYS" ] && DO_CLEANUP=1

    SIZE_FILE=Sources
    contype=application/x-gzip
    conenc=
    ;;

*.dsc)
    resolve_url "$REQUEST"
    # Only if it doesn't exist.
    [ -f $FRONT ] || STREAM="`fetch_file_start $URL_REST $FRONT_BASE $BACKS`"
    [ -n "$CLEANUP_DAYS" ] && DO_CLEANUP=1

    SIZE_FILE=Sources
    contype=text/plain
    conenc=
    ;;

*)
    bad_url "$REQUEST" Unknown extension.
    ;;
esac

# Beware race.
if [ -f "$STREAM" ]; then
    if [ -n "$SIZE_FILE" ]; then
	# Damn.  Don't hit server again to get length: backtrace and find.
	BASEFILE_NAME=`basename $REQUEST`
	case $SIZE_FILE in
	Packages)
	    SIZE=`find_and_cat Packages $FRONT_BASE $URL_SUBDIR | grep-dctrl -e -F Filename "/$BASEFILE_NAME$" -s Size | cut -d\  -f2`
	    ;;
	Sources)
	    PACKAGE_NAME="`basename $REQUEST | cut -d_ -f1`"
	    SIZE=`find_and_cat Sources $FRONT_BASE $URL_SUBDIR | grep-dctrl -X -P  "$PACKAGE_NAME" -s Files | fgrep $BASEFILE_NAME | cut -d\  -f3`
	    ;;
	esac
    fi
else
    # Race: wait for lock to vanish (may have to move file)
    while [ -f $FRONT.lock ]; do sleep 1; done

    if [ -f $FRONT ]; then
	# File already exists in entirity.  Give local answers.
	# wc -c avoids symlink problem.
	SIZE=`wc -c < $FRONT`
	DATE="`date -u -R -r $FRONT`"
	# Turn date into seconds. 
	FILE_DATE=`date -u '+%s' -d "$DATE"`
	if [ -n "$IMS_DATE" ] && [ $IMS_DATE -ge $FILE_DATE ]; then
	    write_header "304 HIT"
	    exit 0
	fi
    else
	# Golly!
	CAUSE=`fgrep -v -x 'client: nothing to do' $FRONT.fail 2>/dev/null`
	rm -f $FRONT.fail
	bad_url $REQUEST "${CAUSE:-was not found on this server}"
	exit 1
    fi
fi

write_header "200 OK" "$contype" "$SIZE" "$DATE" "$conenc" "$REQUEST"
stream_file "$STREAM" "$FRONT"

# If we close fd with data still pending, we send a RST.  If we do
# that without having sent all data out, other end gets short read.
# Drain next request.
(while read LINE && [ "`echo x$LINE | tr -d '\015'`" != x ]; do :; done) &
sleep 1

# Now, derive the uncompressed file.
if [ -n "$UNCOMPRESS" ]; then
    gunzip < $FRONT > $UNCOMPRESS
    touch -r $FRONT $UNCOMPRESS
fi

[ -n "$DO_CLEANUP" ] && cleanup_old $FRONT $CLEANUP_DAYS

# Do async once in a very long while (fails if $CLEAN_SWEEP not set).
if [ -n "$CLEAN_SWEEP" ]; then
    if [ ! -f $FRONT_BASE/.apt-proxy ] ||
	find $FRONT_BASE/.apt-proxy -mtime +$CLEAN_SWEEP |
	    fgrep -q -x $FRONT_BASE/.apt-proxy
    then
	my_lockfile $FRONT_BASE/.apt-proxy &&
	    sweep_clean_unlock $FRONT_BASE $CLEAN_SWEEP &
    fi
fi

exit 0
